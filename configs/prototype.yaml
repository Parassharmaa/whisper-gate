model:
  whisper_size: tiny         # 39M params, 4 encoder layers
  freeze_whisper: true       # only train pulse params
  n_frequencies: 64
  alpha_init: 0.01
  pulse_layers: all          # one per encoder layer

training:
  dataset: librispeech-10h   # 10h subset for fast iteration
  gap_augmentation: true     # train WITH silence gaps
  gap_fractions: [0.0, 0.05, 0.15]
  batch_size: 16
  lr: 1.0e-3
  max_epochs: 10
  fp16: true
  seed: 42

eval:
  gap_levels: [0, 5, 15, 30, multi]
  test_set: test-clean
  hallucination_test: true
